import sys
import os
import re

if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

from langchain_openai import ChatOpenAI
from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent
from langchain.agents.agent_types import AgentType
from langchain.memory import ConversationBufferMemory
from langchain.schema import OutputParserException
from src.config.settings import OPENROUTER_API_KEY, AGENT_CONFIG, OPENROUTER_BASE_URL
import pandas as pd
import streamlit as st
from dotenv import load_dotenv

load_dotenv()

import logging

# Configura logger global
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

def clean_llm_text(text: str) -> str:
    if text is None:
        return ""
    text = re.sub(r"‚ö†Ô∏è.*\n?", "", str(text))
    text = re.sub(r"\[LIMIT_REACHED\].*\n?", "", text, flags=re.IGNORECASE)
    text = "\n".join(line for line in text.splitlines() if not line.strip().startswith("Thought:"))
    return text.strip()

class EDAAgent:
    def __init__(self, df: pd.DataFrame):
        """
        Inicializa agente com OpenAI e mem√≥ria completa persistente
        """
        if df is None:
            raise ValueError("DataFrame (df) n√£o pode ser None.")
        if not isinstance(df, pd.DataFrame):
            try:
                df = pd.DataFrame(df)
            except Exception as e:
                raise ValueError(f"N√£o foi poss√≠vel converter para DataFrame: {e}")

        self.df = df.copy()
                
        # Hist√≥rico estruturado de an√°lises
        self.analysis_history = []
        
        # Informa√ß√µes do dataset (guardadas na mem√≥ria)
        self.dataset_info = self._prepare_dataset_info()
        
        # Inicializa LLM OpenAI
        self.llm = ChatOpenAI(
            model=AGENT_CONFIG['model'],
            temperature=AGENT_CONFIG["temperature"],
            max_tokens=AGENT_CONFIG["max_tokens"],
            openai_api_key=OPENROUTER_API_KEY,
            openai_api_base=OPENROUTER_BASE_URL
            )
        
        # Cria agente pandas com mem√≥ria
        self.agent = create_pandas_dataframe_agent(
            llm=self.llm,
            df=self.df,
            agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True,
            allow_dangerous_code=True,
            max_iterations=80,
            prefix=f"""Voc√™ √© um Agente Especialista em EDA (Exploratory Data Analysis) e sabe gerar gr√°ficos muito eficientes.
                    Objetivo: analisar um ou mais arquivos CSV e gerar um relat√≥rio t√©cnico de alta precis√£o

INFORMACOES DO DATASET:
{self.dataset_info}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
MISS√ÉO: An√°lise R√°pida + Respostas Diretas + Insights de Neg√≥cio
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üéØ MODO DE OPERA√á√ÉO:

1. AN√ÅLISE AUTOM√ÅTICA (Invis√≠vel ao usu√°rio)
   ‚Üí Ao inicializar, voc√™ J√Å analisou o dataset em background
   ‚Üí Estat√≠sticas, correla√ß√µes, outliers, padr√µes: TUDO j√° calculado
   ‚Üí N√ÉO mostre essas an√°lises automaticamente, apenas armazene

2. RESPOSTAS DIRETAS (Quando usu√°rio pergunta)
   ‚Üí Pergunta simples? Resposta simples e objetiva
   ‚Üí Pergunta complexa? Resposta estruturada com insights
   ‚Üí SEMPRE cite n√∫meros espec√≠ficos (ex: "427 outliers", "correla√ß√£o de 0.82")
   ‚Üí NUNCA seja vago (evite "alguns", "v√°rios", "parece")

3. INSIGHTS DE NEG√ìCIO (Al√©m dos n√∫meros)
   ‚Üí Traduza estat√≠sticas em impacto de neg√≥cio
   ‚Üí Identifique oportunidades e riscos
   ‚Üí Recomende a√ß√µes pr√°ticas baseadas em dados

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä FORMATO DE RESPOSTA POR TIPO DE PERGUNTA:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

PERGUNTA DIRETA (ex: "Qual a m√©dia de X?")
‚Üí RESPOSTA: "A m√©dia de X √© 245.67. Valor acima da mediana (180.32), indicando distribui√ß√£o assim√©trica."

PERGUNTA EXPLORAT√ìRIA (ex: "Existem outliers?")
‚Üí ESTRUTURA:
   ‚Ä¢ M√©todo: IQR (Q1-1.5√óIQR, Q3+1.5√óIQR)
   ‚Ä¢ Resultado: 1.847 outliers detectados (0.65% dos dados)
   ‚Ä¢ Colunas afetadas: 'amount' (1.203), 'time' (644)
   ‚Ä¢ Impacto: Outliers concentrados em transa√ß√µes acima de $1.000
   ‚Ä¢ Recomenda√ß√£o: Investigar manualmente transa√ß√µes > $5.000

PERGUNTA COMPLEXA (ex: "Analise correla√ß√µes")
‚Üí ESTRUTURA:
   ‚úì RESUMO: 3 correla√ß√µes fortes identificadas (|r|>0.7)
   ‚úì PRINCIPAIS:
     - V17 √ó Class: r=-0.326 (negativa moderada)
     - V14 √ó Class: r=-0.303 (indicador de fraude)
     - V2 √ó V5: r=0.345 (colinearidade detectada)
   ‚úì INSIGHT: Vari√°veis V17 e V14 s√£o preditores-chave de fraudes
   ‚úì A√á√ÉO: Priorizar essas features em modelos de ML

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üîç AN√ÅLISES QUE VOC√ä DOMINA:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úì ESTAT√çSTICAS DESCRITIVAS
  df.describe(), m√©dia, mediana, moda, desvio padr√£o, vari√¢ncia, quartis

‚úì DETEC√á√ÉO DE OUTLIERS
  M√©todo IQR: Q1-1.5√óIQR e Q3+1.5√óIQR
  Z-score: valores com |z|>3

‚úì CORRELA√á√ïES
  Pearson, Spearman, identifica√ß√£o de multicolinearidade

‚úì PADR√ïES TEMPORAIS
  Tend√™ncias, sazonalidade, agrupamentos por tempo

‚úì DISTRIBUI√á√ïES
  Normalidade, assimetria, curtose, testes estat√≠sticos

‚úì QUALIDADE DE DADOS
  Nulos, duplicatas, inconsist√™ncias, tipos incorretos

‚úì SEGMENTA√á√ÉO
  Clusters naturais, perfis de comportamento, outliers contextuais

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ö° REGRAS DE EXECU√á√ÉO:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úì SEMPRE execute c√≥digo para validar n√∫meros (use df dispon√≠vel)
‚úì SEMPRE cite valores espec√≠ficos (n√£o arredonde demais: 2 decimais OK)
‚úì SEMPRE explique metodologia usada em 1 linha
‚úì SEMPRE traduza para impacto de neg√≥cio quando relevante
‚úì M√°ximo 300 palavras por resposta (exceto relat√≥rio final)

‚úó NUNCA mostre an√°lises longas automaticamente
‚úó NUNCA invente n√∫meros sem calcular
‚úó NUNCA use termos vagos ("alguns", "bastante", "parece")
‚úó NUNCA ignore contexto de neg√≥cio

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìà GR√ÅFICOS (Apenas quando solicitado ou essencial)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Se usu√°rio pedir visualiza√ß√£o:
‚Üí Use matplotlib/seaborn
‚Üí Explique ANTES: "Gerando [tipo de gr√°fico] para [objetivo]"
‚Üí Explique DEPOIS: Interprete o padr√£o visual

Tipos recomendados:
- Distribui√ß√£o ‚Üí Histograma
- Correla√ß√£o ‚Üí Heatmap
- Compara√ß√£o ‚Üí Boxplot
- Temporal ‚Üí Line plot
- Outliers ‚Üí Scatter + Boxplot

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìÑ RELAT√ìRIO FINAL (Apenas se solicitado explicitamente)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Quando usu√°rio pedir "RELAT√ìRIO FINAL COMPLETO":

[START_REPORT]

# RELAT√ìRIO DE AN√ÅLISE EXPLORAT√ìRIA DE DADOS

## 1. RESUMO EXECUTIVO
- S√≠ntese em 3-5 linhas
- Principais achados numerados

## 2. CARACTERIZA√á√ÉO DO DATASET
- Dimens√µes e tipos de dados
- Qualidade (nulos, duplicatas, inconsist√™ncias)
- Estat√≠sticas-chave por coluna

## 3. DESCOBERTAS PRINCIPAIS
### 3.1 Padr√µes Identificados
- Liste padr√µes com evid√™ncias num√©ricas

### 3.2 Correla√ß√µes e Rela√ß√µes
- Correla√ß√µes fortes com interpreta√ß√£o
- Depend√™ncias entre vari√°veis

### 3.3 Anomalias e Outliers
- Quantidade, localiza√ß√£o, poss√≠veis causas
- Impacto nos resultados

## 4. INSIGHTS DE NEG√ìCIO
- Tradu√ß√£o de cada descoberta t√©cnica em valor de neg√≥cio
- Oportunidades identificadas
- Riscos detectados

## 5. RECOMENDA√á√ïES
- A√ß√µes priorit√°rias baseadas em dados
- Pr√≥ximos passos para investiga√ß√£o
- Melhorias sugeridas

## 6. CONCLUS√ÉO
- S√≠ntese final objetiva

[END_REPORT]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üöÄ COMECE AGORA:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Voc√™ j√° analisou o dataset. Aguarde perguntas do usu√°rio.
Responda de forma T√âCNICA, OBJETIVA e com INSIGHTS DE NEG√ìCIO.
"""
)
        
        try:
            st.success(f"[OK] Agente inicializado com {AGENT_CONFIG['model']}")
        except:
            pass

# --- Helper: limpar texto do LLM quando houver tokens de controle ---
    def clean_llm_text(text: str) -> str:
        """
    Remove emojis, tokens de controle e linhas de 'Thought' que podem
    quebrar o output parser. Retorna texto limpo.
    """
        if text is None:
         return ""
    # Remover emoji ‚ö†Ô∏è e qualquer ocorr√™ncia similar
        text = re.sub(r"‚ö†Ô∏è.*\n?", "", text)
    # Remover o token [LIMIT_REACHED] e a linha inteira
        text = re.sub(r"\[LIMIT_REACHED\].*\n?", "", text, flags=re.IGNORECASE)
    # Remover linhas que come√ßam com "Thought:" (se estiverem presentes)
        text = "\n".join(line for line in text.splitlines() if not line.strip().startswith("Thought:"))
        return text.strip()

    def _prepare_dataset_info(self) -> str:
        """Prepara informa√ß√µes compactas do dataset (string), semelhante ao original."""
        numeric_cols = self.df.select_dtypes(include="number").columns.tolist()
        categorical_cols = self.df.select_dtypes(include=["object", "category"]).columns.tolist()

        info = (
            f"Dataset carregado:\n"
            f"- Total de linhas: {len(self.df):,}\n"
            f"- Total de colunas: {len(self.df.columns)}\n"
            f"- Colunas num√©ricas ({len(numeric_cols)}): {', '.join(numeric_cols) if numeric_cols else 'Nenhuma'}\n"
            f"- Colunas categ√≥ricas ({len(categorical_cols)}): {', '.join(categorical_cols) if categorical_cols else 'Nenhuma'}\n"
            f"- Mem√≥ria: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\n"
            f"- Valores nulos (total): {int(self.df.isnull().sum().sum())}\n"
            f"- Duplicatas (linhas): {int(self.df.duplicated().sum())}\n\n"
            f"Primeiras linhas:\n{self.df.head(3).to_string()}\n"
        )
        return info

    def ask(self, query: str) -> str:
        """
        Processa pergunta com contexto completo:
        - cria enhanced_query incluindo hist√≥rico sum√°rio
        - executa agent.run(...) e trata OutputParserException
        - salva hist√≥rico e persiste na mem√≥ria
        """
        try:
            context = self._get_analysis_summary()

            enhanced_query = f"""
HIST√ìRICO:
{context}

PERGUNTA:
{query}

Responda de forma completa usando o contexto quando relevante.
"""
            # Preferimos .run porque create_pandas_dataframe_agent devolve executor com run
            try:
                raw_output = self.agent.run(enhanced_query)
            except OutputParserException as e:
                raw_output = str(e)
            except Exception as e:
                # Alguns agentes podem expor .invoke(...) ‚Äî fallback
                try:
                    resp = self.agent.invoke({"input": enhanced_query})
                    raw_output = resp.get("output", str(resp))
                except Exception as e2:
                    raise e2

            # Detectar token de limite sem limpar, para UI saber que precisa CONTINUAR
            if "[LIMIT_REACHED]" in str(raw_output).upper() or "‚ö†Ô∏è" in str(raw_output):
                result = str(raw_output)
            else:
                result = clean_llm_text(str(raw_output))

            # Salva no hist√≥rico (curto)
            self.analysis_history.append({
                "query": query,
                "response": result,
                "timestamp": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
            })

            # Salva na mem√≥ria (tratamento compat√≠vel)
            try:
                if hasattr(self.memory, "save_context"):
                    self.memory.save_context({"input": query}, {"output": result})
                else:
                    # fallback: append to buffer-like attribute se existir
                    if hasattr(self.memory, "buffer"):
                        self.memory.buffer.append({"input": query, "output": result})
            except Exception as e:
                logger.warning("Falha ao salvar na mem√≥ria: %s", e)

            return result

        except Exception as e:
            error_msg = f"[ERRO] {str(e)}"
            logger.exception("Erro em EDAAgent.ask: %s", e)
            return error_msg

    def _get_analysis_summary(self) -> str:
        """Resumo das √∫ltimas 5 an√°lises (texto curto)."""
        if not self.analysis_history:
            return "Nenhuma analise anterior."
        summary = []
        for i, item in enumerate(self.analysis_history[-5:], 1):
            snippet = item["response"][:200].replace("\n", " ")
            summary.append(f"{i}. {item['query']}: {snippet}...")
        return "\n".join(summary)

    def get_conclusions(self) -> str:
        """
        Gera conclus√µes consolidadas baseadas no hist√≥rico.
        Chama o LLM diretamente para gerar o relat√≥rio final.
        """
        if not self.analysis_history:
            return "Nenhuma analise realizada. Fa√ßa perguntas primeiro!"

        # Constr√≥i o prompt com o hist√≥rico completo (cuidado com token length)
        full_summary = f"DATASET:\n{self.dataset_info}\n\nHIST√ìRICO COMPLETO ({len(self.analysis_history)} an√°lises):\n"
        for i, item in enumerate(self.analysis_history, 1):
            full_summary += f"\n--- ANALISE {i} ({item['timestamp']}) ---\nPergunta: {item['query']}\nResposta: {item['response']}\n"

        conclusion_prompt = f"""
{full_summary}

Gere relatorio COMPLETO e DETALHADO com:

1. RESUMO EXECUTIVO
2. DESCOBERTAS (padr√µes, tend√™ncias, anomalias)
3. CORRELA√á√ïES RELEVANTES
4. QUALIDADE DOS DADOS (nulos, outliers)
5. INSIGHTS E RECOMENDA√á√ïES
6. CONCLUS√ÉO FINAL

Seja ESPEC√çFICO com N√öMEROS das an√°lises.
"""